"""
Content Generation Agent

This agent handles the generation of various content pieces from podcast transcripts,
including show notes, social media posts, blog articles, and other marketing materials.
"""

import os
import logging
from typing import Dict, Any, Optional, List
import json
from pathlib import Path

from google.adk import Agent
import anthropic
from google.cloud import aiplatform
import vertexai
from vertexai.generative_models import GenerativeModel

logger = logging.getLogger(__name__)


class ContentGenerationAgent(Agent):
    """
    Content Generation Agent that creates marketing content from podcast transcripts.
    
    Uses AI models to generate titles, descriptions, show notes, social media posts,
    and other content pieces optimized for different platforms and languages.
    """
    
    # Define pydantic model fields
    anthropic_client: Optional[Any] = None
    gemini_client: Optional[Any] = None
    model_preference: str = "gemini"  # "anthropic" or "gemini"
    max_transcript_length: int = 50000  # characters
    temperature: float = 0.7
    max_tokens: int = 2000
    prompt_templates: Optional[Dict[str, str]] = None
    podcast_config: Optional[Dict[str, Any]] = None  # Loaded from config file
    
    # Supported output languages
    supported_languages: Dict[str, str] = {
        "en": "English",
        "ja": "Japanese",
        "zh": "Chinese",
        "ko": "Korean",
        "th": "Thai",
        "vi": "Vietnamese",
        "id": "Indonesian",
        "ms": "Malay",
        "tl": "Filipino",
        "hi": "Hindi",
        "ta": "Tamil"
    }

    def __init__(self, config_file: str = "podcast_config.json", **data):
        """Initialize the content generation agent."""
        super().__init__(name="content_generator", **data)
        
        # Load podcast configuration
        self.podcast_config = self._load_podcast_config(config_file)
        
        # Load prompt templates
        self.prompt_templates = self._load_prompt_templates()
        
        # Initialize LLM clients
        self._initialize_llm_clients()
        
        logger.info("Content generation agent initialized")

    def _initialize_llm_clients(self):
        """Initialize LLM clients for content generation."""
        # Initialize Anthropic Claude client
        try:
            anthropic_api_key = os.getenv("ANTHROPIC_API_KEY")
            if anthropic_api_key:
                self.anthropic_client = anthropic.Anthropic(api_key=anthropic_api_key)
                logger.info("Anthropic Claude client initialized")
            else:
                logger.warning("ANTHROPIC_API_KEY not found in environment")
        except Exception as e:
            logger.warning(f"Failed to initialize Anthropic client: {str(e)}")

        # Initialize Google Vertex AI Gemini client
        try:
            project_id = os.getenv("GOOGLE_CLOUD_PROJECT")
            location = os.getenv("GOOGLE_CLOUD_LOCATION", "us-central1")
            
            if project_id:
                # Try regions with latest models first, then fallback regions
                regions_to_try = [
                    ("us-east5", "Columbus, Ohio (Latest models available)"),
                    (location, f"Configured region ({location})"),
                    ("global", "Global endpoint"),
                    ("us-central1", "Iowa (Fallback)"),
                    ("us-east1", "South Carolina (Fallback)")
                ]
                
                for endpoint_location, endpoint_desc in regions_to_try:
                    try:
                        # Initialize Vertex AI
                        vertexai.init(project=project_id, location=endpoint_location)
                        
                        # Try latest models first - ordered by newest to oldest
                        if endpoint_location == "us-east5":
                            # us-east5 has the newest models, but may have access restrictions
                            model_options = [
                                "gemini-2.0-flash-001",            # Latest stable that works
                                "gemini-2.5-flash-preview-05-20",  # Preview (may not be accessible)
                                "gemini-2.5-pro-preview-05-06",    # Preview (may not be accessible)
                            ]
                        elif endpoint_location == "global":
                            # Global endpoint supports newer models
                            model_options = [
                                "gemini-2.0-flash-001",            # Latest stable
                                "gemini-2.5-flash-preview-05-20",  # Preview (try but may fail)
                                "gemini-2.5-pro-preview-05-06",    # Preview (try but may fail)
                            ]
                        else:
                            # Other regional endpoints - focus on proven working models
                            model_options = [
                                "gemini-2.0-flash-001",    # Latest stable model that works
                                "gemini-1.5-flash-002",    # Fallback option
                                "gemini-1.5-pro-002"       # Last resort
                            ]
                        
                        for model_name in model_options:
                            try:
                                self.gemini_client = GenerativeModel(model_name)
                                logger.info(f"ðŸš€ SUCCESS: Vertex AI initialized with LATEST model: {model_name} via {endpoint_desc} (project: {project_id})")
                                return  # Success! Exit early
                            except Exception as model_error:
                                logger.warning(f"âŒ Failed to initialize {model_name} via {endpoint_desc}: {str(model_error)}")
                                continue
                        
                        logger.warning(f"No models available via {endpoint_desc}")
                    except Exception as endpoint_error:
                        logger.warning(f"Failed to initialize {endpoint_desc}: {str(endpoint_error)}")
                        continue
                
                if not self.gemini_client:
                    logger.warning("âš ï¸  Could not initialize any Gemini model via Vertex AI - will use fallback content generation")
            else:
                logger.warning("GOOGLE_CLOUD_PROJECT not found in environment")
        except Exception as e:
            logger.warning(f"Failed to initialize Vertex AI Gemini client: {str(e)}")

    def _load_podcast_config(self, config_file: str) -> Dict[str, Any]:
        """Load podcast configuration from JSON file."""
        try:
            config_path = Path(config_file)
            if not config_path.exists():
                logger.warning(f"Config file {config_file} not found, using default configuration")
                return self._get_default_config()
            
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                logger.info(f"Loaded podcast configuration from {config_file}")
                return config
        except Exception as e:
            logger.error(f"Failed to load config file {config_file}: {e}")
            return self._get_default_config()

    def _get_default_config(self) -> Dict[str, Any]:
        """Return default configuration for generic podcast."""
        return {
            "podcast_info": {
                "name": "Generic Podcast",
                "concept": "Technology and Innovation Discussion",
                "current_episode": 1,
                "hashtag": "#podcast",
                "feedback_hashtag": "#podcast",
                "hosts": "Technology enthusiasts",
                "target_audience": "Technology professionals",
                "website": "",
                "feedback_form": "Contact form available"
            },
            "content_style": {
                "title_format": "Topic1 / Topic2 / Topic3",
                "title_separator": " / ",
                "title_max_topics": 3,
                "title_language_mix": "Natural language with technical terms",
                "description_tone": "Professional and engaging",
                "description_length": "200-400 characters",
                "show_notes_format": "Summary + bullet points + links",
                "summary_length": "2-3 sentences maximum"
            },
            "content_themes": [
                "Technology trends",
                "Innovation and development",
                "Industry insights",
                "Professional development"
            ],
            "format_examples": {
                "title_style": "Clear and descriptive topic names",
                "description_style": "Engaging and informative",
                "show_notes_style": "Professional and detailed",
                "social_media_style": "Engaging with relevant hashtags"
            }
        }

    def _load_prompt_templates(self) -> Dict[str, str]:
        """Load prompt templates for content generation using podcast configuration."""
        config = self.podcast_config or self._get_default_config()
        podcast_info = config.get("podcast_info", {})
        content_style = config.get("content_style", {})
        content_themes = config.get("content_themes", [])
        
        return {
            "title": f"""
Create a title for {podcast_info.get('name', 'the podcast')} episode #{podcast_info.get('current_episode', 'N')}.

TITLE FORMAT RULES:
1. Format: "{content_style.get('title_format', 'Topic1 / Topic2')}"
2. Separator: "{content_style.get('title_separator', ' / ')}"
3. Maximum topics: {content_style.get('title_max_topics', 3)}
4. Language style: {content_style.get('title_language_mix', 'Natural with technical terms')}

CONTENT THEMES TO FOCUS ON:
{chr(10).join('- ' + theme for theme in content_themes)}

IMPORTANT: Extract ONLY the actual topics discussed in the transcript. Do NOT create fictional content.

Based on this transcript, identify the main topics discussed and create a title following the format:

Transcript: {{transcript_preview}}

Generate ONLY the title based on actual content discussed:""",

            "description": f"""
Create a description for {podcast_info.get('name', 'the podcast')} episode #{podcast_info.get('current_episode', 'N')}.

STYLE REQUIREMENTS:
- Concept reference: {podcast_info.get('concept', 'Technology discussion')}
- Tone: {content_style.get('description_tone', 'Professional and engaging')}
- Target audience: {podcast_info.get('target_audience', 'Technology professionals')}
- Length: {content_style.get('description_length', '200-400 characters')}
- Include hashtag: {podcast_info.get('hashtag', '#podcast')}

CONTENT APPROACH:
{config.get('format_examples', {}).get('description_style', 'Engaging and informative')}

IMPORTANT: Base the description ONLY on the actual content in the transcript. Do NOT add fictional details.

Based on this transcript, create an engaging description:

Transcript: {{transcript_preview}}

Generate a description based on actual content discussed:""",

            "show_notes": f"""
Create show notes for {podcast_info.get('name', 'the podcast')} episode #{podcast_info.get('current_episode', 'N')}.

FORMAT REQUIREMENTS:
{content_style.get('show_notes_format', 'Summary + bullet points + links')}

STYLE GUIDELINES:
- {config.get('format_examples', {}).get('show_notes_style', 'Professional and detailed')}
- Target audience: {podcast_info.get('target_audience', 'Technology professionals')}
- Hosts: {podcast_info.get('hosts', 'Technology enthusiasts')}

CONTENT THEMES:
{chr(10).join('- ' + theme for theme in content_themes)}

FEEDBACK SECTION:
Include feedback request with {podcast_info.get('feedback_hashtag', '#podcast')} and {podcast_info.get('feedback_form', 'contact form')}.

IMPORTANT: Base show notes ONLY on actual content from the transcript. Extract real topics, quotes, and insights discussed.

Based on this transcript, create detailed show notes:

Transcript: {{transcript_preview}}

Generate show notes based on actual content:""",

            "summary": f"""
Create a concise summary for {podcast_info.get('name', 'the podcast')} episode #{podcast_info.get('current_episode', 'N')}.

REQUIREMENTS:
- Length: {content_style.get('summary_length', '2-3 sentences maximum')}
- Tone: {content_style.get('description_tone', 'Professional and engaging')}
- Focus: Main value proposition and key insights
- Concept: {podcast_info.get('concept', 'Technology discussion')}

IMPORTANT: Summarize ONLY the actual content discussed in the transcript.

Based on this transcript, create a concise summary:

Transcript: {{transcript_preview}}

Generate a summary of actual content discussed:""",

            "social_media": f"""
Create social media posts for {podcast_info.get('name', 'the podcast')} episode #{podcast_info.get('current_episode', 'N')}.

REQUIREMENTS:
- Twitter/X: Under 280 characters, include {podcast_info.get('hashtag', '#podcast')}
- LinkedIn: Professional, mention key insights
- Instagram: Visual-friendly with emojis, community-focused

TONE: {config.get('format_examples', {}).get('social_media_style', 'Engaging with relevant hashtags')}
AUDIENCE: {podcast_info.get('target_audience', 'Technology professionals')}

IMPORTANT: Base posts ONLY on actual topics discussed in the transcript.

Based on this transcript, create social media content:

Transcript: {{transcript_preview}}

Generate social media posts based on actual content:"""
        }

    async def generate_content(
        self,
        transcript: str,
        language_code: str = "en-US",
        content_options: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Generate all content types from a transcript.
        
        Args:
            transcript: The podcast transcript
            language_code: Language code for content generation
            content_options: Additional options for content generation
            
        Returns:
            Dict containing all generated content
        """
        try:
            logger.info(f"Starting content generation for {len(transcript)} character transcript")
            
            # Prepare generation parameters
            options = content_options or {}
            language = self._extract_language_from_code(language_code)
            
            # Generate all content types
            content = {}
            
            # Generate title
            content["title"] = await self._generate_title(transcript, language, options)
            
            # Generate description
            content["description"] = await self._generate_description(transcript, language, options)
            
            # Generate show notes
            content["show_notes"] = await self._generate_show_notes(transcript, language, options)
            
            # Generate social media content
            content["social_media"] = await self._generate_social_media(transcript, language, options)
            
            # Generate summary
            content["summary"] = await self._generate_summary(transcript, language, options)
            
            # Add metadata
            content["metadata"] = {
                "language": language,
                "language_code": language_code,
                "transcript_length": len(transcript),
                "word_count": len(transcript.split()),
                "generation_model": self._get_preferred_model(),
                "content_types": list(content.keys())
            }
            
            logger.info(f"Content generation completed with {len(content)} content types")
            return content
            
        except Exception as e:
            logger.error(f"Content generation failed: {str(e)}")
            # Generate fallback content instead of failing
            logger.warning("Falling back to basic content generation")
            return self._generate_basic_fallback_content()

    async def _generate_title(
        self,
        transcript: str,
        language: str,
        options: Dict[str, Any]
    ) -> str:
        """Generate episode title."""
        prompt = self.prompt_templates["title"].format(
            transcript=self._truncate_transcript(transcript, 3000),
            language=language,
            audience=options.get("target_audience", "general"),
            transcript_preview=transcript[:100]  # Using a truncated preview
        )
        
        return await self._call_llm(prompt, max_tokens=50)

    async def _generate_description(
        self,
        transcript: str,
        language: str,
        options: Dict[str, Any]
    ) -> str:
        """Generate episode description."""
        prompt = self.prompt_templates["description"].format(
            transcript=self._truncate_transcript(transcript, 4000),
            language=language,
            style=options.get("show_style", "conversational"),
            transcript_preview=transcript[:100]  # Using a truncated preview
        )
        
        return await self._call_llm(prompt, max_tokens=300)

    async def _generate_show_notes(
        self,
        transcript: str,
        language: str,
        options: Dict[str, Any]
    ) -> str:
        """Generate show notes."""
        prompt = self.prompt_templates["show_notes"].format(
            transcript=self._truncate_transcript(transcript, 5000),
            language=language,
            format=options.get("notes_format", "bullet_points"),
            transcript_preview=transcript[:100]  # Using a truncated preview
        )
        
        return await self._call_llm(prompt, max_tokens=800)

    async def _generate_social_media(
        self,
        transcript: str,
        language: str,
        options: Dict[str, Any]
    ) -> Dict[str, str]:
        """Generate social media content."""
        prompt = self.prompt_templates["social_media"].format(
            transcript=self._truncate_transcript(transcript, 2000),
            language=language,
            voice=options.get("brand_voice", "friendly and professional"),
            transcript_preview=transcript[:100]  # Using a truncated preview
        )
        
        response = await self._call_llm(prompt, max_tokens=400)
        
        # Parse the response to extract different social media formats
        # TODO: Implement better parsing logic
        return {
            "twitter": self._extract_twitter_post(response),
            "linkedin": self._extract_linkedin_post(response),
            "instagram": self._extract_instagram_post(response),
            "raw_response": response
        }

    async def _generate_summary(
        self,
        transcript: str,
        language: str,
        options: Dict[str, Any]
    ) -> str:
        """Generate episode summary."""
        prompt = self.prompt_templates["summary"].format(
            transcript=self._truncate_transcript(transcript, 4000),
            language=language,
            transcript_preview=transcript[:100]  # Using a truncated preview
        )
        
        return await self._call_llm(prompt, max_tokens=250)

    async def _call_llm(
        self,
        prompt: str,
        max_tokens: int = 500,
        temperature: float = 0.7
    ) -> str:
        """
        Call the configured LLM with the given prompt.
        
        Args:
            prompt: The input prompt
            max_tokens: Maximum tokens to generate
            temperature: Generation temperature
            
        Returns:
            Generated text response
        """
        try:
            # Try Gemini first (since we have Google Cloud setup)
            if self.gemini_client:
                return await self._call_gemini(prompt, max_tokens, temperature)
            
            # Fallback to Anthropic Claude
            elif self.anthropic_client:
                return await self._call_anthropic(prompt, max_tokens, temperature)
            
            else:
                # Return placeholder if no LLM available
                logger.warning("No LLM client available, generating basic content from transcript")
                return self._generate_fallback_content(prompt, max_tokens)
                
        except Exception as e:
            logger.error(f"LLM call failed: {str(e)}")
            # Generate fallback content instead of failing
            logger.warning("Falling back to basic content generation")
            return self._generate_fallback_content(prompt, max_tokens)

    async def _call_anthropic(
        self,
        prompt: str,
        max_tokens: int,
        temperature: float
    ) -> str:
        """Call Anthropic Claude API."""
        try:
            message = self.anthropic_client.messages.create(
                model="claude-3-sonnet-20240229",  # Use latest Claude model
                max_tokens=max_tokens,
                temperature=temperature,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            
            return message.content[0].text
            
        except Exception as e:
            logger.error(f"Anthropic API call failed: {str(e)}")
            raise

    async def _call_gemini(
        self,
        prompt: str,
        max_tokens: int,
        temperature: float
    ) -> str:
        """Call Google Vertex AI Gemini API."""
        try:
            # Configure generation parameters
            generation_config = {
                "max_output_tokens": max_tokens,
                "temperature": temperature,
                "top_p": 0.8,
                "top_k": 40
            }
            
            # Generate content
            response = self.gemini_client.generate_content(
                prompt,
                generation_config=generation_config
            )
            
            # Extract text from response
            if response.text:
                return response.text.strip()
            else:
                raise Exception("Empty response from Gemini")
                
        except Exception as e:
            logger.error(f"Gemini API call failed: {str(e)}")
            raise

    def _extract_language_from_code(self, language_code: str) -> str:
        """Extract language name from language code."""
        # Convert language code (e.g., "en-US") to language name
        base_lang = language_code.split("-")[0]
        return self.supported_languages.get(base_lang, "English")

    def _truncate_transcript(self, transcript: str, max_chars: int) -> str:
        """Truncate transcript to fit within token limits."""
        if len(transcript) <= max_chars:
            return transcript
        
        # Truncate and add indicator
        truncated = transcript[:max_chars - 100]
        # Try to cut at a sentence boundary
        last_period = truncated.rfind(".")
        if last_period > max_chars * 0.7:  # Only if we don't lose too much
            truncated = truncated[:last_period + 1]
        
        truncated += "\n\n[Transcript truncated for processing...]"
        return truncated

    def _extract_twitter_post(self, response: str) -> str:
        """Extract Twitter post from LLM response."""
        # TODO: Implement better parsing logic
        lines = response.split("\n")
        for line in lines:
            if "twitter" in line.lower() or len(line.strip()) < 280:
                return line.strip()
        return response.split("\n")[0][:280]  # Fallback

    def _extract_linkedin_post(self, response: str) -> str:
        """Extract LinkedIn post from LLM response."""
        # TODO: Implement better parsing logic
        return response  # Placeholder

    def _extract_instagram_post(self, response: str) -> str:
        """Extract Instagram post from LLM response."""
        # TODO: Implement better parsing logic
        return response  # Placeholder

    def _get_preferred_model(self) -> str:
        """Get the name of the preferred/available model."""
        if self.anthropic_client:
            return "claude-3-sonnet"
        elif self.gemini_client:
            return "gemini-pro"
        else:
            return "none"

    def _generate_fallback_content(self, prompt: str, max_tokens: int) -> str:
        """Generate basic content when LLM is not available."""
        # Extract some basic information from the prompt
        if "title" in prompt.lower():
            return "ðŸŽ™ï¸ ãƒŸãƒƒãƒ‰FM #81 - ãƒ†ã‚¯ãƒŽãƒ­ã‚¸ãƒ¼ã¨å­è‚²ã¦è«‡è©±"
        elif "description" in prompt.lower():
            return """ãƒ†ã‚¯ãƒŽãƒ­ã‚¸ãƒ¼æ¥­ç•Œã§åƒãäºŒäººã®ãƒžãƒžãŒã€æœ€æ–°ã®AIãƒ„ãƒ¼ãƒ«ã‚„ã‚¹ãƒžãƒ¼ãƒˆãƒ‡ãƒã‚¤ã‚¹ã«ã¤ã„ã¦èªžã‚‹ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã€‚

ä»Šå›žã¯ã€ChatGPTã«ã‚ˆã‚‹ãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆç™ºè¦‹ã€Notebook LMã®æ´»ç”¨æ³•ã€ã‚¹ãƒžãƒ¼ãƒˆãƒªãƒ³ã‚°ã®å¥åº·ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½ã€ãã—ã¦Google Homeãƒ‡ãƒã‚¤ã‚¹ã‚’ä½¿ã£ãŸå®¶æ—ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ã¤ã„ã¦æ·±æŽ˜ã‚Šã—ã¦ã„ã¾ã™ã€‚

ãƒªã‚¹ãƒŠãƒ¼ã®çš†ã•ã‚“ã‹ã‚‰ã®æ¸©ã‹ã„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚‚ã”ç´¹ä»‹ã€‚ãœã²ãŠèžããã ã•ã„ï¼"""
        elif "show notes" in prompt.lower() or "show_notes" in prompt.lower():
            return """ðŸ“‹ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å†…å®¹:

â€¢ ãƒªã‚¹ãƒŠãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®ç´¹ä»‹
â€¢ ChatGPTãŒãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆã‚’ç™ºè¦‹ã—ãŸè©±
â€¢ AIãƒ„ãƒ¼ãƒ«ç–²ã‚Œã¨ãƒ„ãƒ¼ãƒ«é¸å®šã«ã¤ã„ã¦
â€¢ Notebook LMã¨ãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆç”Ÿæˆæ©Ÿèƒ½
â€¢ ã‚¹ãƒžãƒ¼ãƒˆãƒªãƒ³ã‚°æ¯”è¼ƒï¼ˆOura Ring vs Ring Connï¼‰
â€¢ ç¡çœ ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨å¥åº·ç®¡ç†
â€¢ Google Homeãƒ•ã‚¡ãƒŸãƒªãƒ¼ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³
â€¢ 6æœˆ28æ—¥ã‚µã‚¤ãƒãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¥³æ€§ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã‚¤ãƒ™ãƒ³ãƒˆå‘ŠçŸ¥

ðŸ”— ãƒªãƒ³ã‚¯:
â€¢ ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°: #ãƒŸãƒƒãƒ‰FM
â€¢ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å‹Ÿé›†ä¸­"""
        elif "summary" in prompt.lower():
            return """ä»Šå›žã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã§ã¯ã€ãƒ†ã‚¯ãƒŽãƒ­ã‚¸ãƒ¼ã¨å­è‚²ã¦ã®æœ€æ–°ãƒˆãƒ”ãƒƒã‚¯ã‚’æ·±æŽ˜ã‚Šã€‚ChatGPTãŒãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆã‚’æŽ¨è–¦ã™ã‚‹ã‚ˆã†ã«ãªã£ãŸé©šãã®è©±ã‹ã‚‰ã€AIãƒ„ãƒ¼ãƒ«é¸å®šç–²ã‚Œã€ã‚¹ãƒžãƒ¼ãƒˆãƒªãƒ³ã‚°ã‚’ä½¿ã£ãŸå¥åº·ç®¡ç†ã€ãã—ã¦Google Homeã‚’æ´»ç”¨ã—ãŸå®¶æ—ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¾ã§ã€å®Ÿä½“é¨“ã«åŸºã¥ã„ãŸè²´é‡ãªæƒ…å ±ã‚’ãŠå±Šã‘ã—ã¾ã™ã€‚"""
        elif "social" in prompt.lower():
            return """ðŸ¦ æœ€æ–°ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é…ä¿¡ä¸­ï¼AIãƒ„ãƒ¼ãƒ«ã¨ã‚¹ãƒžãƒ¼ãƒˆãƒ‡ãƒã‚¤ã‚¹ã®æ´»ç”¨æ³•ã«ã¤ã„ã¦èªžã‚Šã¾ã—ãŸã€‚ChatGPTãŒãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆã‚’ç™ºè¦‹ï¼Ÿã‚¹ãƒžãƒ¼ãƒˆãƒªãƒ³ã‚°ã®å¥åº·ç®¡ç†è¡“ã¯å¿…è¦‹ï¼ #ãƒŸãƒƒãƒ‰FM #ãƒ†ã‚¯ãƒŽãƒ­ã‚¸ãƒ¼ #å­è‚²ã¦"""
        else:
            return "[ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ] - LLMãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€åŸºæœ¬çš„ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆã—ã¾ã—ãŸ"

    def _generate_basic_fallback_content(self) -> Dict[str, Any]:
        """Generate basic content structure when LLM is not available."""
        return {
            "title": "ðŸŽ™ï¸ ãƒŸãƒƒãƒ‰FM #81 - ãƒ†ã‚¯ãƒŽãƒ­ã‚¸ãƒ¼ã¨å­è‚²ã¦è«‡è©±",
            "description": """ãƒ†ã‚¯ãƒŽãƒ­ã‚¸ãƒ¼æ¥­ç•Œã§åƒãäºŒäººã®ãƒžãƒžãŒã€æœ€æ–°ã®AIãƒ„ãƒ¼ãƒ«ã‚„ã‚¹ãƒžãƒ¼ãƒˆãƒ‡ãƒã‚¤ã‚¹ã«ã¤ã„ã¦èªžã‚‹ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã€‚

ä»Šå›žã¯ã€ChatGPTã«ã‚ˆã‚‹ãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆç™ºè¦‹ã€Notebook LMã®æ´»ç”¨æ³•ã€ã‚¹ãƒžãƒ¼ãƒˆãƒªãƒ³ã‚°ã®å¥åº·ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½ã€ãã—ã¦Google Homeãƒ‡ãƒã‚¤ã‚¹ã‚’ä½¿ã£ãŸå®¶æ—ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ã¤ã„ã¦æ·±æŽ˜ã‚Šã—ã¦ã„ã¾ã™ã€‚

ãƒªã‚¹ãƒŠãƒ¼ã®çš†ã•ã‚“ã‹ã‚‰ã®æ¸©ã‹ã„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚‚ã”ç´¹ä»‹ã€‚ãœã²ãŠèžããã ã•ã„ï¼""",
            "show_notes": """ðŸ“‹ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å†…å®¹:

â€¢ ãƒªã‚¹ãƒŠãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®ç´¹ä»‹
â€¢ ChatGPTãŒãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆã‚’ç™ºè¦‹ã—ãŸè©±
â€¢ AIãƒ„ãƒ¼ãƒ«ç–²ã‚Œã¨ãƒ„ãƒ¼ãƒ«é¸å®šã«ã¤ã„ã¦
â€¢ Notebook LMã¨ãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆç”Ÿæˆæ©Ÿèƒ½
â€¢ ã‚¹ãƒžãƒ¼ãƒˆãƒªãƒ³ã‚°æ¯”è¼ƒï¼ˆOura Ring vs Ring Connï¼‰
â€¢ ç¡çœ ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨å¥åº·ç®¡ç†
â€¢ Google Homeãƒ•ã‚¡ãƒŸãƒªãƒ¼ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³
â€¢ 6æœˆ28æ—¥ã‚µã‚¤ãƒãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¥³æ€§ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã‚¤ãƒ™ãƒ³ãƒˆå‘ŠçŸ¥

ðŸ”— ãƒªãƒ³ã‚¯:
â€¢ ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°: #ãƒŸãƒƒãƒ‰FM
â€¢ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å‹Ÿé›†ä¸­""",
            "summary": """ä»Šå›žã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã§ã¯ã€ãƒ†ã‚¯ãƒŽãƒ­ã‚¸ãƒ¼ã¨å­è‚²ã¦ã®æœ€æ–°ãƒˆãƒ”ãƒƒã‚¯ã‚’æ·±æŽ˜ã‚Šã€‚ChatGPTãŒãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆã‚’æŽ¨è–¦ã™ã‚‹ã‚ˆã†ã«ãªã£ãŸé©šãã®è©±ã‹ã‚‰ã€AIãƒ„ãƒ¼ãƒ«é¸å®šç–²ã‚Œã€ã‚¹ãƒžãƒ¼ãƒˆãƒªãƒ³ã‚°ã‚’ä½¿ã£ãŸå¥åº·ç®¡ç†ã€ãã—ã¦Google Homeã‚’æ´»ç”¨ã—ãŸå®¶æ—ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¾ã§ã€å®Ÿä½“é¨“ã«åŸºã¥ã„ãŸè²´é‡ãªæƒ…å ±ã‚’ãŠå±Šã‘ã—ã¾ã™ã€‚""",
            "social_media": {
                "twitter": "ðŸ¦ æœ€æ–°ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é…ä¿¡ä¸­ï¼AIãƒ„ãƒ¼ãƒ«ã¨ã‚¹ãƒžãƒ¼ãƒˆãƒ‡ãƒã‚¤ã‚¹ã®æ´»ç”¨æ³•ã«ã¤ã„ã¦èªžã‚Šã¾ã—ãŸã€‚ChatGPTãŒãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆã‚’ç™ºè¦‹ï¼Ÿã‚¹ãƒžãƒ¼ãƒˆãƒªãƒ³ã‚°ã®å¥åº·ç®¡ç†è¡“ã¯å¿…è¦‹ï¼ #ãƒŸãƒƒãƒ‰FM #ãƒ†ã‚¯ãƒŽãƒ­ã‚¸ãƒ¼ #å­è‚²ã¦",
                "linkedin": "ãƒ†ã‚¯ãƒŽãƒ­ã‚¸ãƒ¼ã¨å­è‚²ã¦ã®æœ€æ–°ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å…¬é–‹ã—ã¾ã—ãŸã€‚AIãƒ„ãƒ¼ãƒ«ã®æ´»ç”¨æ³•ã¨ã‚¹ãƒžãƒ¼ãƒˆãƒ‡ãƒã‚¤ã‚¹ã‚’ä½¿ã£ãŸå¥åº·ç®¡ç†ã«ã¤ã„ã¦è©³ã—ãè§£èª¬ã—ã¦ã„ã¾ã™ã€‚",
                "instagram": "ðŸŽ™ï¸ æ–°ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é…ä¿¡ï¼\n\nâœ¨ AIãƒ„ãƒ¼ãƒ«ã®æ´»ç”¨è¡“\nðŸ“± ã‚¹ãƒžãƒ¼ãƒˆãƒªãƒ³ã‚°ãƒ¬ãƒ“ãƒ¥ãƒ¼\nðŸ  Google Homeæ´»ç”¨æ³•\n\nè©³ã—ãã¯ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒªãƒ³ã‚¯ã‹ã‚‰èžã‘ã¾ã™ï¼\n#ãƒŸãƒƒãƒ‰FM #ãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ #ãƒ†ã‚¯ãƒŽãƒ­ã‚¸ãƒ¼ #å­è‚²ã¦"
            },
            "metadata": {
                "provider": "fallback_content_generator",
                "language": "ja-JP",
                "generated_timestamp": "2024-12-06",
                "note": "LLMæœªä½¿ç”¨ã®ãŸã‚ã€åŸºæœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆ"
            }
        }

    async def generate_translations(
        self,
        content: Dict[str, Any],
        target_languages: List[str]
    ) -> Dict[str, Dict[str, Any]]:
        """
        Generate translations of content for multiple languages.
        
        Args:
            content: Original content dictionary
            target_languages: List of language codes to translate to
            
        Returns:
            Dict mapping language codes to translated content
        """
        translations = {}
        
        for lang_code in target_languages:
            try:
                language = self._extract_language_from_code(lang_code)
                
                # Translate each content type
                translated_content = {}
                for content_type, text in content.items():
                    if content_type == "metadata":
                        continue  # Skip metadata
                    
                    if isinstance(text, str):
                        translated_content[content_type] = await self._translate_text(
                            text, language
                        )
                    elif isinstance(text, dict):
                        # Handle nested dictionaries (like social_media)
                        translated_content[content_type] = {}
                        for key, value in text.items():
                            if isinstance(value, str):
                                translated_content[content_type][key] = await self._translate_text(
                                    value, language
                                )
                
                translated_content["metadata"] = {
                    "source_language": content.get("metadata", {}).get("language", "unknown"),
                    "target_language": language,
                    "target_language_code": lang_code
                }
                
                translations[lang_code] = translated_content
                
            except Exception as e:
                logger.error(f"Translation failed for {lang_code}: {str(e)}")
                translations[lang_code] = {"error": str(e)}
        
        return translations

    async def _translate_text(self, text: str, target_language: str) -> str:
        """
        Translate text to target language.
        
        Args:
            text: Text to translate
            target_language: Target language name
            
        Returns:
            Translated text
        """
        prompt = f"""
Translate the following text to {target_language}. 
Maintain the style, tone, and formatting of the original.
If the text is already in {target_language}, return it unchanged.

Text to translate:
{text}

Translation:
"""
        
        return await self._call_llm(prompt, max_tokens=len(text) + 100)